{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Vorlesung 'Syntax natürlicher Sprachen'***\n",
    "\n",
    "--- \n",
    "# Vorlesung 3: Syntaktische Kategorien\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.parse.generate import generate\n",
    "import itertools\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 1. Tagsets\n",
    "\n",
    "---\n",
    "\n",
    "### POS-Tagsets:\n",
    "\n",
    "- **Universal POS-Tagset**: https://universaldependencies.org/u/pos/\n",
    "  - **NLTK-Version (reduziert)**: http://www.nltk.org/book/ch05.html#tab-universal-tagset\n",
    " \n",
    "\n",
    "- **Penn Treebank POS-Tagset**: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "\n",
    "- **TIGER/STTS-POS-Tagset**: https://www.linguistik.hu-berlin.de/de/institut/professuren/korpuslinguistik/mitarbeiter-innen/hagen/STTS_Tagset_Tiger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Konstituenten-Tagsets:\n",
    "\n",
    "- **Penn Treebank Constituent Tags**: http://surdeanu.cs.arizona.edu//mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html\n",
    "- **TIGER Konstituenten Labels**: https://www.linguistik.hu-berlin.de/de/institut/professuren/korpuslinguistik/mitarbeiter-innen/hagen/Tiger_Knotenlabels\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dependency-Tagsets:\n",
    "- **UD Dependency Labels**: https://universaldependencies.org/u/dep/all.html\n",
    "- **TIGER Dependency Labels**: https://www.linguistik.hu-berlin.de/de/institut/professuren/korpuslinguistik/mitarbeiter-innen/hagen/DDB_edge\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels & Sources für spaCy Modelle:\n",
    "\n",
    "- **english model**: https://github.com/explosion/spacy-models/releases/tag/en_core_web_sm-3.1.0\n",
    "    - training corpus: OntoNotes (Penn-Treebank-Style-Constituents > transformation to CLEAR Dependency Labels \n",
    "    - **CLEAR Dependencies**: ähnlich **Stanford Dependencies** (Vorgänger von Universal Dependencies)\n",
    "\n",
    "- **german model**: https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-3.1.0\n",
    "    - training corpus: **TIGER** Corpus (konstituentenannotiert > dependenztransformiert = Tiger2Dep)\n",
    "   \n",
    "   \n",
    "- in Hilfsfunktion: auch UD-Label enthalten (für andere Modelle)    \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen für Erklärung von Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('DT') #Penn-Treebank-POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: determiner/pronoun, singular\n",
      "    this each another that 'nother\n"
     ]
    }
   ],
   "source": [
    "nltk.help.brown_tagset('DT') #Brown-POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AT: article\n",
      "    the an no a every th' ever' ye\n"
     ]
    }
   ],
   "source": [
    "nltk.help.brown_tagset('AT') #Brown-POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adposition\n",
      "conjunction, subordinating or preposition\n",
      "preposition; circumposition left\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('ADP'), #UD-POS\n",
    "      spacy.explain('IN'), #PENN-POS\n",
    "      spacy.explain('APPR'), #TIGER-POS\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepositional phrase\n",
      "adjective phrase\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('PP'), #PENN-Constituent\n",
    "      spacy.explain('ADJP'),#PENN-Constituent\n",
    "      spacy.explain('AP'),#TIGER-Constituent: nicht in spacy enthalten!\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun kernel element\n",
      "case marking\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('nk'), #TIGER-Dep\n",
    "      spacy.explain('case'), #UD-Dep\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Generierung Satzschemata (POS-Muster) mit Phrasenstruktur-Regeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl an POS-Mustern:  \n",
      "\tbei 3 Regelanwendungen: 2 \n",
      "\tbei 4 Regelanwendungen: 14 \n",
      "\tbei 5 Regelanwendungen: 14\n"
     ]
    }
   ],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    VP -> 'V' | 'V' NP | 'V' NP NP\n",
    "    NP -> 'DET' 'N' | 'N'\n",
    "\"\"\")\n",
    "\n",
    "#Generierung (http://www.nltk.org/howto/generate.html):\n",
    "from nltk.parse.generate import generate\n",
    "print('Anzahl an POS-Mustern: ', \n",
    "    '\\n\\tbei 3 Regelanwendungen:', len(list(generate(grammar, depth=3))), \n",
    "    '\\n\\tbei 4 Regelanwendungen:', len(list(generate(grammar, depth=4))),\n",
    "    '\\n\\tbei 5 Regelanwendungen:', len(list(generate(grammar, depth=5))))                                                     \n",
    "                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DET N V\n",
      "N V\n"
     ]
    }
   ],
   "source": [
    "for sentence in generate(grammar, depth=3):\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DET N V\n",
      "DET N V DET N\n",
      "DET N V N\n",
      "DET N V DET N DET N\n",
      "DET N V DET N N\n",
      "DET N V N DET N\n",
      "DET N V N N\n",
      "N V\n",
      "N V DET N\n",
      "N V N\n",
      "N V DET N DET N\n",
      "N V DET N N\n",
      "N V N DET N\n",
      "N V N N\n"
     ]
    }
   ],
   "source": [
    "for sentence in generate(grammar, depth=4):\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generierung Satzschemata (POS-Muster) mit rekursiver Phrasenstruktur-Regel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl an POS-Mustern:  \n",
      "\tbei 6 Regelanwendungen: 84 \n",
      "\tbei 7 Regelanwendungen: 1204 \n",
      "\tbei 8 Regelanwendungen: 103716\n"
     ]
    }
   ],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> NP PP\n",
    "    PP -> 'P' NP\n",
    "    NP -> 'DET' 'N'\n",
    "    NP -> 'N'\n",
    "    VP -> 'V' NP \n",
    "\"\"\")\n",
    "\n",
    "#Generierung:\n",
    "from nltk.parse.generate import generate\n",
    "print('Anzahl an POS-Mustern: ', \n",
    "    '\\n\\tbei 6 Regelanwendungen:', len(list(generate(grammar, depth=6))), \n",
    "    '\\n\\tbei 7 Regelanwendungen:', len(list(generate(grammar, depth=7))),\n",
    "    '\\n\\tbei 8 Regelanwendungen:', len(list(generate(grammar, depth=8))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DET N P DET N P DET N V DET N P DET N\n",
      "DET N P DET N P DET N V DET N P N\n",
      "DET N P DET N P DET N V N P DET N\n",
      "DET N P DET N P DET N V N P N\n",
      "DET N P DET N P DET N V DET N\n",
      "DET N P DET N P DET N V N\n",
      "DET N P DET N P N V DET N P DET N\n",
      "DET N P DET N P N V DET N P N\n",
      "DET N P DET N P N V N P DET N\n",
      "DET N P DET N P N V N P N\n",
      "DET N P DET N P N V DET N\n",
      "DET N P DET N P N V N\n",
      "DET N P N P DET N V DET N P DET N\n",
      "DET N P N P DET N V DET N P N\n",
      "DET N P N P DET N V N P DET N\n",
      "DET N P N P DET N V N P N\n",
      "DET N P N P DET N V DET N\n",
      "DET N P N P DET N V N\n",
      "DET N P N P N V DET N P DET N\n",
      "DET N P N P N V DET N P N\n",
      "DET N P N P N V N P DET N\n",
      "DET N P N P N V N P N\n",
      "DET N P N P N V DET N\n",
      "DET N P N P N V N\n",
      "N P DET N P DET N V DET N P DET N\n",
      "N P DET N P DET N V DET N P N\n",
      "N P DET N P DET N V N P DET N\n",
      "N P DET N P DET N V N P N\n",
      "N P DET N P DET N V DET N\n",
      "N P DET N P DET N V N\n",
      "N P DET N P N V DET N P DET N\n",
      "N P DET N P N V DET N P N\n",
      "N P DET N P N V N P DET N\n",
      "N P DET N P N V N P N\n",
      "N P DET N P N V DET N\n",
      "N P DET N P N V N\n",
      "N P N P DET N V DET N P DET N\n",
      "N P N P DET N V DET N P N\n",
      "N P N P DET N V N P DET N\n",
      "N P N P DET N V N P N\n",
      "N P N P DET N V DET N\n",
      "N P N P DET N V N\n",
      "N P N P N V DET N P DET N\n",
      "N P N P N V DET N P N\n",
      "N P N P N V N P DET N\n",
      "N P N P N V N P N\n",
      "N P N P N V DET N\n",
      "N P N P N V N\n",
      "DET N P DET N V DET N P DET N\n",
      "DET N P DET N V DET N P N\n",
      "DET N P DET N V N P DET N\n",
      "DET N P DET N V N P N\n",
      "DET N P DET N V DET N\n",
      "DET N P DET N V N\n",
      "DET N P N V DET N P DET N\n",
      "DET N P N V DET N P N\n",
      "DET N P N V N P DET N\n",
      "DET N P N V N P N\n",
      "DET N P N V DET N\n",
      "DET N P N V N\n",
      "N P DET N V DET N P DET N\n",
      "N P DET N V DET N P N\n",
      "N P DET N V N P DET N\n",
      "N P DET N V N P N\n",
      "N P DET N V DET N\n",
      "N P DET N V N\n",
      "N P N V DET N P DET N\n",
      "N P N V DET N P N\n",
      "N P N V N P DET N\n",
      "N P N V N P N\n",
      "N P N V DET N\n",
      "N P N V N\n",
      "DET N V DET N P DET N\n",
      "DET N V DET N P N\n",
      "DET N V N P DET N\n",
      "DET N V N P N\n",
      "DET N V DET N\n",
      "DET N V N\n",
      "N V DET N P DET N\n",
      "N V DET N P N\n",
      "N V N P DET N\n",
      "N V N P N\n",
      "N V DET N\n",
      "N V N\n"
     ]
    }
   ],
   "source": [
    "for sentence in generate(grammar, depth=6):\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Distributionsanalyse (Feststellung von Wortklassen)\n",
    "\n",
    "### Suche kontextäquivalenter Wörter im Korpus mit NLTK (paradigmatische Dimension)\n",
    "\n",
    "http://www.nltk.org/book/ch05.html#using-a-tagger:\n",
    "\n",
    "> Lexical categories like \"noun\" and part-of-speech tags like NN seem to have their uses, but the details will be obscure to many readers. You might wonder what justification there is for introducing this extra level of information. Many of these categories arise from superficial analysis the distribution of words in text. Consider the following analysis involving *woman* (a noun), *bought* (a verb), *over* (a preposition), and *the* (a determiner). The `text.similar()` method takes a word *w*, finds all contexts *w1 w w2*, then finds all words *w'* that appear in the same context, i.e. *w1 w' w2*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.nltk.org/book/ch05.html#using-a-tagger\n",
    "\n",
    "from nltk.corpus import brown\n",
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Observe that searching for *woman* finds nouns; searching for *bought* mostly finds verbs; searching for *over* generally finds prepositions; searching for *the* finds several determiners. A tagger can correctly identify the tags on these words in the context of a sentence, e.g. *The woman bought over $150,000 worth of clothes.* (http://www.nltk.org/book/ch05.html#using-a-tagger)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day year car moment world house family child country boy\n",
      "state job place way war girl work word\n"
     ]
    }
   ],
   "source": [
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made said done put had seen found given left heard was been brought\n",
      "set got that took in told felt\n"
     ]
    }
   ],
   "source": [
    "text.similar('bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in on to of and for with from at by that into as up out down through\n",
      "is all about\n"
     ]
    }
   ],
   "source": [
    "text.similar('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a his this their its her an that our any all one these my in your no\n",
      "some other and\n"
     ]
    }
   ],
   "source": [
    "text.similar('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 4. Distribution von Konstituenten\n",
    "\n",
    "### Suche nach nominalen POS-Mustern (Syntagmen) im Korpus  \n",
    "\n",
    "http://www.nltk.org/book/ch05.html#nouns:\n",
    "\n",
    "> Let's inspect some tagged text to see what parts of speech occur before a noun, with the most frequent ones first. To begin with, we construct a list of bigrams whose members are themselves word-tag pairs such as `(('The', 'DET'), ('Fulton', 'NP'))` and `(('Fulton', 'NP'), ('County', 'N'))`. Then we construct a `FreqDist` from the tag parts of the bigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 7959),\n",
       " ('DET', 7373),\n",
       " ('ADJ', 4761),\n",
       " ('ADP', 3781),\n",
       " ('.', 2796),\n",
       " ('VERB', 1842),\n",
       " ('CONJ', 938),\n",
       " ('NUM', 894),\n",
       " ('ADV', 186),\n",
       " ('PRT', 94),\n",
       " ('PRON', 19),\n",
       " ('X', 11)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
    "word_tag_pairs = nltk.bigrams(brown_news_tagged)\n",
    "noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == 'NOUN']\n",
    "fdist = nltk.FreqDist(noun_preceders)\n",
    "[(tag, fq) for (tag, fq) in fdist.most_common()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This confirms our assertion that nouns occur after determiners and adjectives, including numeral adjectives (tagged as `NUM``). (http://www.nltk.org/book/ch05.html#nouns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Adjektive als Klasse distributionsäquivalenter Wörter:\n",
    "\n",
    "#### Suche nach distributionsäquivalenten Wörtern (Auftreten in gleichen Kontexten):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little new first good small large great the old other strong young\n",
      "major white second short beautiful a best long\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "text.similar('big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wortarten-Kontexte von distributionsäquivalenter Wörtern (als Vertreter einer Distributionsklasse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('DET', 'NOUN'), 193),\n",
       " (('ADP', 'NOUN'), 42),\n",
       " (('VERB', 'NOUN'), 36),\n",
       " (('DET', 'ADJ'), 30),\n",
       " (('DET', 'NUM'), 22),\n",
       " (('NOUN', 'NOUN'), 13),\n",
       " (('DET', 'ADP'), 13),\n",
       " (('ADJ', 'NOUN'), 12),\n",
       " (('CONJ', 'NOUN'), 10),\n",
       " (('NUM', 'NOUN'), 10),\n",
       " (('NOUN', 'ADJ'), 8),\n",
       " (('ADV', 'NOUN'), 8),\n",
       " (('VERB', '.'), 7),\n",
       " (('.', 'NOUN'), 6),\n",
       " (('DET', '.'), 5),\n",
       " (('ADV', 'ADP'), 5),\n",
       " (('ADV', '.'), 5),\n",
       " (('NOUN', '.'), 4),\n",
       " (('VERB', 'VERB'), 4),\n",
       " (('DET', 'VERB'), 4)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rechter und linker Kontext für eine Menge distributionsäquivalenter Wörter:\n",
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
    "word_tag_trigrams = nltk.trigrams(brown_news_tagged)\n",
    "adj_contexts = [(a[1], c[1]) for (a, b, c) in word_tag_trigrams if b[0] in ('big', 'little', 'new', 'first', 'good', 'small', 'large', 'great')]\n",
    "\n",
    "fdist = nltk.FreqDist(adj_contexts)\n",
    "[(tag, fq) for (tag, fq) in fdist.most_common(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('DET', 'NOUN'), 2081),\n",
       " (('ADP', 'NOUN'), 745),\n",
       " (('NOUN', 'NOUN'), 368),\n",
       " (('ADJ', 'NOUN'), 363),\n",
       " (('VERB', 'NOUN'), 351),\n",
       " (('.', 'NOUN'), 332),\n",
       " (('DET', 'ADJ'), 218),\n",
       " (('CONJ', 'NOUN'), 214),\n",
       " (('ADV', 'NOUN'), 154),\n",
       " (('VERB', 'ADP'), 145),\n",
       " (('NUM', 'NOUN'), 129),\n",
       " (('DET', '.'), 104),\n",
       " (('ADV', 'ADP'), 100),\n",
       " (('ADV', '.'), 83),\n",
       " (('DET', 'NUM'), 83),\n",
       " (('VERB', '.'), 77),\n",
       " (('VERB', 'PRT'), 61),\n",
       " (('.', 'ADP'), 57),\n",
       " (('DET', 'CONJ'), 56),\n",
       " (('NOUN', 'ADP'), 56)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linker und rechter Kontext für ADJ:\n",
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
    "word_tag_trigrams = nltk.trigrams(brown_news_tagged)\n",
    "adj_contexts = [(a[1], c[1]) for (a, b, c) in word_tag_trigrams if b[1] == 'ADJ']\n",
    "\n",
    "fdist = nltk.FreqDist(adj_contexts)\n",
    "[(tag, fq) for (tag, fq) in fdist.most_common(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Konstituententests\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konstituententests 1: Eliminierung\n",
    "\n",
    "####  vor allem zur Feststellung nicht-notwendiger Elemente\n",
    "\n",
    "##### Kriterium: Erhalt der Wohlgeformtheit bei Löschung (= Substitution mit leerem Wort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['sehr große', 'Hund']\n",
      "1 ['der', 'Hund']\n",
      "2 ['der', 'sehr große']\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"der\", \"sehr große\", \"Hund\"]\n",
    "\n",
    "sentencelist = []\n",
    "for i in range(len((sentence))):\n",
    "    sentencelist.append(sentence.copy())\n",
    "    sentencelist[i].pop(i)\n",
    "    print(i, sentencelist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['der', 'Hund']\n"
     ]
    }
   ],
   "source": [
    "#Erhalt der Wohlgeformtheit:\n",
    "print(i, sentencelist[1])\n",
    "    # 'sehr große' als weglassbare Konstituente (ADJ-Phrase ADJP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konstituententests 2: Substitution\n",
    "\n",
    "#### Substituierbarkeit als Evidenz für Vorliegen einer syntaktischen Einheit gleicher Kategorie (Konstituente)\n",
    "\n",
    "   ##### Austauschbarkeit im gleichen Kontext\n",
    "\n",
    "##### Kriterium: Erhalt der Wohlgeformtheit bei Ersatz mit kürzerer Einheit \n",
    "- insbesondere Pronomen bei NPs, intransitiven Verben bei VPs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['laufe', 'sehe nichts']\n",
      "1 ['ich', 'laufe']\n"
     ]
    }
   ],
   "source": [
    "# Test: Ersatz VP mit intransitivem Verb\n",
    "sentence = [\"ich\", \"sehe nichts\"]\n",
    "substitute = \"laufe\"\n",
    "\n",
    "sentencelist = []\n",
    "for i in range(len((sentence))):\n",
    "    sentencelist.append(sentence.copy())\n",
    "    sentencelist[i][i] = substitute\n",
    "    print(i, sentencelist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['ich', 'laufe']\n"
     ]
    }
   ],
   "source": [
    "# Erhalt der Wohlgeformtheit:\n",
    "print(1, sentencelist[1])\n",
    "    #'sehe nichts' als Konstituente (VP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konstituententests 3: Permutation\n",
    "\n",
    "#### Verschiebbarkeit als Evidenz für Vorliegen einer syntaktischen Einheit (Konstituente)\n",
    "\n",
    "\n",
    "##### Kriterien für Feststellung als syntaktische Einheit:\n",
    "\n",
    "1. Wortgruppe an anderer Position\n",
    "2. Erhalt der Wohlgeformtheit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('der Mann', 'sieht', 'nichts')\n",
      "1 ('der Mann', 'nichts', 'sieht')\n",
      "2 ('sieht', 'der Mann', 'nichts')\n",
      "3 ('sieht', 'nichts', 'der Mann')\n",
      "4 ('nichts', 'der Mann', 'sieht')\n",
      "5 ('nichts', 'sieht', 'der Mann')\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"der Mann\", \"sieht\", \"nichts\"]\n",
    "\n",
    "permutations = list(itertools.permutations(sentence))\n",
    "for (i, item) in enumerate(permutations):\n",
    "    print(i, item)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('der Mann', 'sieht', 'nichts')\n",
      "('der Mann', 'nichts', 'sieht') IM NEBENSATZKONTEXT\n",
      "('sieht', 'der Mann', 'nichts') IM FRAGEKONTEXT\n",
      "('nichts', 'sieht', 'der Mann')\n"
     ]
    }
   ],
   "source": [
    "#wohlgeformte Sätze:\n",
    "print(list(itertools.permutations(sentence))[0])\n",
    "print(list(itertools.permutations(sentence))[1], \"IM NEBENSATZKONTEXT\")\n",
    "print(list(itertools.permutations(sentence))[2], \"IM FRAGEKONTEXT\")\n",
    "print(list(itertools.permutations(sentence))[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEIN (keine Permutation): ('der Mann', 'sieht', 'nichts')\n",
      "NEIN (keine Permutation der NP): ('der Mann', 'nichts', 'sieht')\n",
      "JA: ('sieht', 'der Mann', 'nichts')\n",
      "JA: ('nichts', 'sieht', 'der Mann')\n"
     ]
    }
   ],
   "source": [
    "#davon Permutationen, die die NP ('der Mann') als Konstituente bestätigen:\n",
    "\n",
    "print(\"NEIN (keine Permutation):\", list(itertools.permutations(sentence))[0])\n",
    "print(\"NEIN (keine Permutation der NP):\", list(itertools.permutations(sentence))[1])\n",
    "print(\"JA:\", list(itertools.permutations(sentence))[2])\n",
    "print(\"JA:\", list(itertools.permutations(sentence))[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEIN (keine Permutation): ('der Mann', 'sieht', 'nichts')\n",
      "NEIN (Wortgruppe nicht gemeinsam verschoben): ('der Mann', 'nichts', 'sieht')\n",
      "NEIN (Wortgruppe nicht gemeinsam verschoben): ('sieht', 'der Mann', 'nichts')\n",
      "NEIN (Wortgruppe nicht gemeinsam verschoben): ('nichts', 'sieht', 'der Mann')\n"
     ]
    }
   ],
   "source": [
    "#Permutationen, die die VP ('sieht nichts') als Konstituente bestätigen:\n",
    "\n",
    "print(\"NEIN (keine Permutation):\", list(itertools.permutations(sentence))[0])\n",
    "print(\"NEIN (Wortgruppe nicht gemeinsam verschoben):\", list(itertools.permutations(sentence))[1])\n",
    "print(\"NEIN (Wortgruppe nicht gemeinsam verschoben):\", list(itertools.permutations(sentence))[2])\n",
    "print(\"NEIN (Wortgruppe nicht gemeinsam verschoben):\", list(itertools.permutations(sentence))[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "098dd75f65594c0f992ddeffeb8523e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "0ccfa5129ab64950adda077f6ac8f44a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "IntSliderModel",
      "state": {
       "layout": "IPY_MODEL_641f3cf31a334ee6810a9a92f93e89a7",
       "style": "IPY_MODEL_63aadafcc3504a3b8b15fdea9d4ec244",
       "value": 50
      }
     },
     "1b37a149a74c40c68d34c61b481048f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "367694b2ecc94715a20952236f9ced88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3d6ebd239fc84a8e95b3007988e38134": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "63aadafcc3504a3b8b15fdea9d4ec244": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "641f3cf31a334ee6810a9a92f93e89a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "993ec756027845c19085347ca5f9f6ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "bb743dc2dd2d4bb08dd2c5255e1d87c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cb4a4abdac584f78ac3b37579ac33b10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "IntSliderModel",
      "state": {
       "layout": "IPY_MODEL_f25c2eb22704411b9981f8047a2c0182",
       "style": "IPY_MODEL_993ec756027845c19085347ca5f9f6ce",
       "value": 50
      }
     },
     "cfbb8ba5969a49e18a1733c50e6b73e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "IntSliderModel",
      "state": {
       "layout": "IPY_MODEL_1b37a149a74c40c68d34c61b481048f8",
       "style": "IPY_MODEL_3d6ebd239fc84a8e95b3007988e38134",
       "value": 50
      }
     },
     "da2e37629b7141b8a194c275f89ccef5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "eafa48b852af4f40b414b8d4f0b90c7a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "IntSliderModel",
      "state": {
       "layout": "IPY_MODEL_da2e37629b7141b8a194c275f89ccef5",
       "style": "IPY_MODEL_bb743dc2dd2d4bb08dd2c5255e1d87c6"
      }
     },
     "f196171317cf493fabedf854f59c1d44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "IntSliderModel",
      "state": {
       "layout": "IPY_MODEL_367694b2ecc94715a20952236f9ced88",
       "style": "IPY_MODEL_098dd75f65594c0f992ddeffeb8523e3"
      }
     },
     "f25c2eb22704411b9981f8047a2c0182": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
